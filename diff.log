diff --git a/conf/lab.mk b/conf/lab.mk
index 57e8d2a..619dbf0 100644
--- a/conf/lab.mk
+++ b/conf/lab.mk
@@ -1,3 +1,3 @@
-LAB=5
-CONFIG_KSPACE=y
+LAB=6
+CONFIG_KSPACE=n
 LABDEFS=-Ddebug=0
diff --git a/grade-lab6 b/grade-lab6
new file mode 100755
index 0000000..b842c31
--- /dev/null
+++ b/grade-lab6
@@ -0,0 +1,17 @@
+#!/usr/bin/env python2
+# -*- coding: utf-8 -*-
+
+from gradelib import *
+
+r = Runner(save("jos.out"),
+           stop_breakpoint("readline"))
+
+@test(0, "running JOS")
+def test_jos():
+    r.run_qemu()
+
+@test(100, "Physical page allocator", parent=test_jos)
+def test_check_page_alloc():
+    r.match(r"check_page_alloc\(\) succeeded!")
+
+run_tests()
diff --git a/inc/env.h b/inc/env.h
index 881e3e6..e36f6e5 100644
--- a/inc/env.h
+++ b/inc/env.h
@@ -8,6 +8,8 @@
 #include <inc/memlayout.h>
 
 typedef int32_t envid_t;
+extern pml4e_t *kern_pml4e;
+extern physaddr_t kern_cr3;
 
 // An environment ID 'envid_t' has three parts:
 //
diff --git a/kern/Makefrag b/kern/Makefrag
index e541415..05ba162 100644
--- a/kern/Makefrag
+++ b/kern/Makefrag
@@ -20,6 +20,7 @@ KERN_SRCFILES :=	kern/entry.S \
 			kern/dwarf.c \
 			kern/dwarf_lines.c \
 			kern/monitor.c \
+			kern/pmap.c \
 			kern/env.c \
 			kern/kclock.c \
 			kern/picirq.c \
diff --git a/kern/init.c b/kern/init.c
index c8b252f..640512b 100644
--- a/kern/init.c
+++ b/kern/init.c
@@ -9,6 +9,7 @@
 #include <kern/monitor.h>
 #include <kern/tsc.h>
 #include <kern/console.h>
+#include <kern/pmap.h>
 #include <kern/env.h>
 #include <kern/timer.h>
 #include <kern/trap.h>
@@ -116,6 +117,11 @@ i386_init(void) {
   cprintf("6828 decimal is %o octal!\n", 6828);
   cprintf("END: %p\n", end);
 
+#ifndef CONFIG_KSPACE
+  // Lab 6 memory management initialization functions
+  mem_init();
+#endif
+
   // Perform global constructor initialisation (e.g. asan)
   // This must be done as early as possible
   extern void (*__ctors_start)();
diff --git a/kern/kclock.c b/kern/kclock.c
index 3f7cc1b..b731cfe 100644
--- a/kern/kclock.c
+++ b/kern/kclock.c
@@ -69,3 +69,20 @@ rtc_check_status(void) {
 
   return status;
 }
+
+unsigned
+mc146818_read(unsigned reg) {
+  outb(IO_RTC_CMND, reg);
+  return inb(IO_RTC_DATA);
+}
+
+void
+mc146818_write(unsigned reg, unsigned datum) {
+  outb(IO_RTC_CMND, reg);
+  outb(IO_RTC_DATA, datum);
+}
+
+unsigned
+mc146818_read16(unsigned reg) {
+  return mc146818_read(reg) | (mc146818_read(reg + 1) << 8);
+}
\ No newline at end of file
diff --git a/kern/kclock.h b/kern/kclock.h
index 0164cae..237bb9c 100644
--- a/kern/kclock.h
+++ b/kern/kclock.h
@@ -6,6 +6,8 @@
 #error "This is a JOS kernel header; user programs should not #include it"
 #endif
 
+#define IO_RTC 0x070 /* RTC port */
+
 #define IO_RTC_CMND 0x070 /* RTC control port */
 #define IO_RTC_DATA 0x071 /* RTC data port */
 
@@ -35,4 +37,28 @@
 void rtc_init(void);
 uint8_t rtc_check_status(void);
 
+#define MC_NVRAM_START 0xe /* start of NVRAM: offset 14 */
+#define MC_NVRAM_SIZE  50  /* 50 bytes of NVRAM */
+
+/* NVRAM bytes 7 & 8: base memory size */
+#define NVRAM_BASELO (MC_NVRAM_START + 7) /* low byte; RTC off. 0x15 */
+#define NVRAM_BASEHI (MC_NVRAM_START + 8) /* high byte; RTC off. 0x16 */
+
+/* NVRAM bytes 9 & 10: extended memory size */
+#define NVRAM_EXTLO (MC_NVRAM_START + 9)  /* low byte; RTC off. 0x17 */
+#define NVRAM_EXTHI (MC_NVRAM_START + 10) /* high byte; RTC off. 0x18 */
+
+/* NVRAM bytes 34 and 35: extended memory POSTed size */
+#define NVRAM_PEXTLO (MC_NVRAM_START + 38) /* low byte; RTC off. 0x34 */
+#define NVRAM_PEXTHI (MC_NVRAM_START + 39) /* high byte; RTC off. 0x35 */
+
+/* NVRAM byte 36: current century.  (please increment in Dec99!) */
+#define NVRAM_CENTURY (MC_NVRAM_START + 36) /* RTC offset 0x32 */
+
+unsigned mc146818_read(unsigned reg);
+void mc146818_write(unsigned reg, unsigned datum);
+unsigned mc146818_read16(unsigned reg);
+
+#define BCD2BIN(bcd) ((((bcd)&15) + ((bcd) >> 4) * 10))
+
 #endif // !JOS_KERN_KCLOCK_H
diff --git a/kern/kdebug.c b/kern/kdebug.c
index 01bd16a..8f5ad45 100644
--- a/kern/kdebug.c
+++ b/kern/kdebug.c
@@ -5,6 +5,8 @@
 #include <inc/elf.h>
 #include <inc/x86.h>
 
+#include <kern/kdebug.h>
+#include <kern/pmap.h>
 #include <kern/env.h>
 #include <kern/kdebug.h>
 #include <inc/uefi.h>
diff --git a/kern/monitor.c b/kern/monitor.c
index 48b7e40..f539d34 100644
--- a/kern/monitor.c
+++ b/kern/monitor.c
@@ -13,6 +13,7 @@
 #include <kern/tsc.h>
 #include <kern/timer.h>
 #include <kern/env.h>
+#include <kern/pmap.h>
 
 #define CMDBUF_SIZE 80 // enough for one VGA text line
 
@@ -25,6 +26,8 @@ struct Command {
 
 // LAB 5: Your code here.
 // Implement timer_start (mon_start), timer_stop (mon_stop), timer_freq (mon_frequency) commands.
+// LAB 6: Your code here.
+// Implement memory (mon_memory) command.
 static struct Command commands[] = {
     {"help", "Display this list of commands", mon_help},
     {"hello", "Display greeting message", mon_hello},
@@ -186,6 +189,10 @@ mon_frequency(int argc, char **argv, struct Trapframe *tf) {
   return 0;
 }
 
+// LAB 6: Your code here.
+// Implement memory (mon_memory) commands.
+
+
 /***** Kernel monitor command interpreter *****/
 
 #define WHITESPACE "\t\r\n "
diff --git a/kern/monitor.h b/kern/monitor.h
index c4f6b35..589f61c 100644
--- a/kern/monitor.h
+++ b/kern/monitor.h
@@ -22,5 +22,6 @@ int mon_mycommand(int argc, char **argv, struct Trapframe *tf);
 int mon_start(int argc, char **argv, struct Trapframe *tf);
 int mon_stop(int argc, char **argv, struct Trapframe *tf);
 int mon_frequency(int argc, char **argv, struct Trapframe *tf);
+int mon_memory(int argc, char **argv, struct Trapframe *tf);
 
 #endif // !JOS_KERN_MONITOR_H
diff --git a/kern/pmap.c b/kern/pmap.c
new file mode 100644
index 0000000..e504252
--- /dev/null
+++ b/kern/pmap.c
@@ -0,0 +1,738 @@
+/* See COPYRIGHT for copyright information. */
+
+#include <inc/x86.h>
+#include <inc/mmu.h>
+#include <inc/error.h>
+#include <inc/string.h>
+#include <inc/assert.h>
+
+#include <kern/pmap.h>
+#include <kern/kclock.h>
+#include <kern/env.h>
+#include <inc/uefi.h>
+
+#ifdef SANITIZE_SHADOW_BASE
+// asan unpoison routine used for whitelisting regions.
+void platform_asan_unpoison(void *addr, uint32_t size);
+// not sanitized memset allows us to access "invalid" areas for extra poisoning.
+void *__nosan_memset(void *, int, size_t);
+#endif
+
+extern uint64_t pml4phys;
+// These variables are set by i386_detect_memory()
+size_t npages;                // Amount of physical memory (in pages)
+static size_t npages_basemem; // Amount of base memory (in pages)
+
+// These variables are set in mem_init()
+pde_t *kern_pml4e;                                 // Kernel's initial page directory
+physaddr_t kern_cr3;                               // Physical address of boot time page directory
+struct PageInfo *pages;                            // Physical page state array
+static struct PageInfo *page_free_list     = NULL; // Free list of physical pages
+static struct PageInfo *page_free_list_top = NULL;
+//Pointers to start and end of UEFI memory map
+EFI_MEMORY_DESCRIPTOR *mmap_base = NULL;
+EFI_MEMORY_DESCRIPTOR *mmap_end  = NULL;
+size_t mem_map_size              = 0;
+
+// --------------------------------------------------------------
+// Detect machine's physical memory setup.
+// --------------------------------------------------------------
+
+//Count all available to the system pages (checks for avaiability
+//should be done in page_init).
+static void
+load_params_read(LOADER_PARAMS *desc, size_t *npages_basemem, size_t *npages_extmem) {
+  EFI_MEMORY_DESCRIPTOR *mmap_curr;
+  size_t num_pages = 0;
+  mem_map_size     = desc->MemoryMapDescriptorSize;
+  mmap_base        = (EFI_MEMORY_DESCRIPTOR *)(uintptr_t)desc->MemoryMap;
+  mmap_end         = (EFI_MEMORY_DESCRIPTOR *)((uintptr_t)desc->MemoryMap + desc->MemoryMapSize);
+
+  for (mmap_curr = mmap_base; mmap_curr < mmap_end; mmap_curr = (EFI_MEMORY_DESCRIPTOR *)((uintptr_t)mmap_curr + mem_map_size)) {
+    num_pages += mmap_curr->NumberOfPages;
+  }
+
+  *npages_basemem = num_pages > (IOPHYSMEM / PGSIZE) ? IOPHYSMEM / PGSIZE : num_pages;
+  *npages_extmem  = num_pages - *npages_basemem;
+}
+
+static void
+i386_detect_memory(void) {
+  size_t npages_extmem;
+  size_t pextmem;
+
+  if (uefi_lp && uefi_lp->MemoryMap) {
+    load_params_read(uefi_lp, &npages_basemem, &npages_extmem);
+  } else {
+    // Use CMOS calls to measure available base & extended memory.
+    // (CMOS calls return results in kilobytes.)
+    npages_basemem = (mc146818_read16(NVRAM_BASELO) * 1024) / PGSIZE;
+    npages_extmem  = (mc146818_read16(NVRAM_EXTLO) * 1024) / PGSIZE;
+    pextmem        = ((size_t)mc146818_read16(NVRAM_PEXTLO) * 1024 * 64);
+    if (pextmem)
+      npages_extmem = ((16 * 1024 * 1024) + pextmem - (1 * 1024 * 1024)) / PGSIZE;
+  }
+
+  // Calculate the number of physical pages available in both base
+  // and extended memory.
+  if (npages_extmem)
+    npages = (EXTPHYSMEM / PGSIZE) + npages_extmem;
+  else
+    npages = npages_basemem;
+
+  cprintf("Physical memory: %luM available, base = %luK, extended = %luK\n",
+          (unsigned long)(npages * PGSIZE / 1024 / 1024),
+          (unsigned long)(npages_basemem * PGSIZE / 1024),
+          (unsigned long)(npages_extmem * PGSIZE / 1024));
+}
+
+//
+//Check if page is allocatable according to saved UEFI MemMap.
+//
+bool
+is_page_allocatable(size_t pgnum) {
+  EFI_MEMORY_DESCRIPTOR *mmap_curr;
+  size_t pg_start, pg_end;
+
+  if (!mmap_base || !mmap_end)
+    return true; //Assume page is allocabale if no loading parameters were passed.
+
+  for (mmap_curr = mmap_base; mmap_curr < mmap_end; mmap_curr = (EFI_MEMORY_DESCRIPTOR *)((uintptr_t)mmap_curr + mem_map_size)) {
+    pg_start = ((uintptr_t)mmap_curr->PhysicalStart >> EFI_PAGE_SHIFT);
+    pg_end   = pg_start + mmap_curr->NumberOfPages;
+
+    if (pgnum >= pg_start && pgnum < pg_end) {
+      switch (mmap_curr->Type) {
+        case EFI_LOADER_CODE:
+        case EFI_LOADER_DATA:
+        case EFI_BOOT_SERVICES_CODE:
+        case EFI_BOOT_SERVICES_DATA:
+        case EFI_CONVENTIONAL_MEMORY:
+          if (mmap_curr->Attribute & EFI_MEMORY_WB)
+            return true;
+          else
+            return false;
+          break;
+        default:
+          return false;
+          break;
+      }
+    }
+  }
+  //Assume page is allocatable if it's not found in MemMap.
+  return true;
+}
+
+// Fix loading params and memory map address to virtual ones.
+static void
+fix_lp_addresses(void) {
+  mmap_base = (EFI_MEMORY_DESCRIPTOR *)(uintptr_t)uefi_lp->MemoryMapVirt;
+  mmap_end  = (EFI_MEMORY_DESCRIPTOR *)((uintptr_t)uefi_lp->MemoryMapVirt + uefi_lp->MemoryMapSize);
+  uefi_lp   = (LOADER_PARAMS *)uefi_lp->SelfVirtual;
+}
+
+// --------------------------------------------------------------
+// Set up memory mappings above UTOP.
+// --------------------------------------------------------------
+
+static void boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm);
+static void check_page_free_list(bool only_low_memory);
+static void check_page_alloc(void);
+static void check_kern_pml4e(void);
+static physaddr_t check_va2pa(pde_t *pgdir, uintptr_t va);
+static void check_page(void);
+static void check_page_installed_pml4(void);
+
+// This simple physical memory allocator is used only while JOS is setting
+// up its virtual memory system.  page_alloc() is the real allocator.
+//
+// If n>0, allocates enough pages of contiguous physical memory to hold 'n'
+// bytes.  Doesn't initialize the memory.  Returns a kernel virtual address.
+//
+// If n==0, returns the address of the next free page without allocating
+// anything.
+//
+// If we're out of memory, boot_alloc should panic.
+// This function may ONLY be used during initialization,
+// before the page_free_list list has been set up.
+static void *
+boot_alloc(uint32_t n) {
+  static char *nextfree; // virtual address of next byte of free memory
+
+  // Initialize nextfree if this is the first time.
+  // 'end' is a magic symbol automatically generated by the linker,
+  // which points to the end of the kernel's bss segment:
+  // the first virtual address that the linker did *not* assign
+  // to any kernel code or global variables.
+
+  // Allocate a chunk large enough to hold 'n' bytes, then update
+  // nextfree.  Make sure nextfree is kept aligned
+  // to a multiple of PGSIZE.
+  //
+  // LAB 6: Your code here.
+
+  (void)nextfree;
+  return NULL;
+}
+
+// Set up a two-level page table:
+//    kern_pml4e is its linear (virtual) address of the root
+//
+// This function only sets up the kernel part of the address space
+// (ie. addresses >= UTOP).  The user part of the address space
+// will be setup later.
+//
+// From UTOP to ULIM, the user is allowed to read but not write.
+// Above ULIM the user cannot read or write.
+void
+mem_init(void) {
+  pml4e_t *pml4e;
+  // Find out how much memory the machine has (npages & npages_basemem).
+  i386_detect_memory();
+
+  // Remove this line when you're ready to test this function.
+  // panic("mem_init: This function is not finished\n");
+
+  //////////////////////////////////////////////////////////////////////
+  // create initial page directory.
+  pml4e = boot_alloc(PGSIZE);
+  memset(pml4e, 0, PGSIZE);
+  kern_pml4e = pml4e;
+  kern_cr3   = PADDR(pml4e);
+
+  //////////////////////////////////////////////////////////////////////
+  // Recursively insert PD in itself as a page table, to form
+  // a virtual page table at virtual address UVPT.
+  // (For now, you don't have understand the greater purpose of the
+  // following line.)
+
+  // Permissions: kernel R, user R
+  kern_pml4e[PML4(UVPT)] = kern_cr3 | PTE_P | PTE_U;
+
+  //////////////////////////////////////////////////////////////////////
+  // Allocate an array of npages 'struct PageInfo's and store it in 'pages'.
+  // The kernel uses this array to keep track of physical pages: for
+  // each physical page, there is a corresponding struct PageInfo in this
+  // array.  'npages' is the number of physical pages in memory.  Use memset
+  // to initialize all fields of each struct PageInfo to 0.
+  // LAB 6: Your code here.
+
+  //////////////////////////////////////////////////////////////////////
+  // Now that we've allocated the initial kernel data structures, we set
+  // up the list of free physical pages. Once we've done so, all further
+  // memory management will go through the page_* functions. In
+  // particular, we can now map memory using boot_map_region
+  // or page_insert
+  page_init();
+
+  check_page_free_list(1);
+  check_page_alloc();
+}
+
+#ifdef SANITIZE_SHADOW_BASE
+void
+kasan_mem_init(void) {
+  // Unpoison memory in which kernel was loaded
+  platform_asan_unpoison((void *)KERNBASE, (uint32_t)(boot_alloc(0) - KERNBASE));
+
+  // Go through all pages and unpoison pages which have at least one ref.
+  for (int pgidx = 0; pgidx < npages; pgidx++) {
+    if (pages[pgidx].pp_ref > 0) {
+      platform_asan_unpoison(page2kva(&pages[pgidx]), PGSIZE);
+    }
+  }
+
+  // Additinally map all UEFI runtime services corresponding shadow memory.
+  EFI_MEMORY_DESCRIPTOR *mmap_curr;
+  struct PageInfo *pg;
+  uintptr_t virt_addr, virt_end;
+
+  for (mmap_curr = mmap_base; mmap_curr < mmap_end; mmap_curr = (EFI_MEMORY_DESCRIPTOR *)((uintptr_t)mmap_curr + mem_map_size)) {
+    virt_addr = ROUNDDOWN((uintptr_t)((mmap_curr->VirtualStart >> 3) +
+                                      SANITIZE_SHADOW_OFF),
+                          PGSIZE);
+    virt_end  = ROUNDUP((uintptr_t)(virt_addr + (mmap_curr->NumberOfPages * PGSIZE >> 3)), PGSIZE);
+    if (mmap_curr->Attribute & EFI_MEMORY_RUNTIME) {
+      for (; virt_addr < virt_end; virt_addr += PGSIZE) {
+        pg = page_alloc(ALLOC_ZERO);
+        if (!pg)
+          panic("region_alloc: page alloc failed!\n");
+
+        if (page_insert(kern_pml4e, pg, (void *)virt_addr,
+                        PTE_P | PTE_W) < 0)
+          panic("Cannot allocate any memory for page directory allocation");
+      }
+    }
+  }
+}
+#endif
+
+// --------------------------------------------------------------
+// Tracking of physical pages.
+// The 'pages' array has one 'struct PageInfo' entry per physical page.
+// Pages are reference counted, and free pages are kept on a linked list.
+// --------------------------------------------------------------
+
+//
+// Initialize page structure and memory free list.
+// After this is done, NEVER use boot_alloc again.  ONLY use the page
+// allocator functions below to allocate and deallocate physical
+// memory via the page_free_list.
+//
+void
+page_init(void) {
+  // The example code here marks all physical pages as free.
+  // However this is not truly the case.  What memory is free?
+  //  1) Mark physical page 0 as in use.
+  //     This way we preserve the real-mode IDT and BIOS structures
+  //     in case we ever need them.  (Currently we don't, but...)
+  //  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE)
+  //     is free.
+  //  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must
+  //     never be allocated.
+  //  4) Then extended memory [EXTPHYSMEM, ...).
+  //     Some of it is in use, some is free. Where is the kernel
+  //     in physical memory?  Which pages are already in use for
+  //     page tables and other data structures?
+  //
+  // Change the code to reflect this.
+  // NB: DO NOT actually touch the physical memory corresponding to
+  // free pages!
+  size_t i;
+  uintptr_t first_free_page;
+  struct PageInfo *last = NULL;
+
+  //Mark physical page 0 as in use.
+  pages[0].pp_ref  = 1;
+  pages[0].pp_link = NULL;
+
+  //  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE)
+  //     is free.
+  pages[2].pp_ref = 0;
+  page_free_list  = &pages[2];
+  last            = &pages[2];
+  for (i = 2; i < npages_basemem; i++) {
+    if (is_page_allocatable(i)) {
+      pages[i].pp_ref = 0;
+      last->pp_link   = &pages[i];
+      last            = &pages[i];
+    } else {
+      pages[i].pp_ref  = 1;
+      pages[i].pp_link = NULL;
+    }
+  }
+
+  //  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must
+  //     never be allocated.
+  first_free_page = PADDR(boot_alloc(0)) / PGSIZE;
+  for (i = npages_basemem; i < first_free_page; i++) {
+    pages[i].pp_ref  = 1;
+    pages[i].pp_link = NULL;
+  }
+
+  //     Some of it is in use, some is free. Where is the kernel
+  //     in physical memory?  Which pages are already in use for
+  //     page tables and other data structures?
+  for (i = first_free_page; i < npages; i++) {
+    if (is_page_allocatable(i)) {
+      pages[i].pp_ref = 0;
+      last->pp_link   = &pages[i];
+      last            = &pages[i];
+    } else {
+      pages[i].pp_ref  = 1;
+      pages[i].pp_link = NULL;
+    }
+  }
+}
+
+//
+// Allocates a physical page.  If (alloc_flags & ALLOC_ZERO), fills the entire
+// returned physical page with '\0' bytes.  Does NOT increment the reference
+// count of the page - the caller must do these if necessary (either explicitly
+// or via page_insert).
+//
+// Be sure to set the pp_link field of the allocated page to NULL so
+// page_free can check for double-free bugs.
+//
+// Returns NULL if out of free memory.
+//
+// Hint: use page2kva and memset
+struct PageInfo *
+page_alloc(int alloc_flags) {
+  if (!page_free_list) {
+    return NULL;
+  }
+  struct PageInfo *return_page = page_free_list;
+  page_free_list               = page_free_list->pp_link;
+  return_page->pp_link         = NULL;
+
+  if (!page_free_list) {
+    page_free_list_top = NULL;
+  }
+
+#ifdef SANITIZE_SHADOW_BASE
+  if ((uintptr_t)page2kva(return_page) >= SANITIZE_SHADOW_BASE) {
+    cprintf("page_alloc: returning shadow memory page! Increase base address?\n");
+    return NULL;
+  }
+  // Unpoison allocated memory before accessing it!
+  platform_asan_unpoison(page2kva(return_page), PGSIZE);
+#endif
+
+  if (alloc_flags & ALLOC_ZERO) {
+    memset(page2kva(return_page), 0, PGSIZE);
+  }
+
+  return return_page;
+}
+
+int
+page_is_allocated(const struct PageInfo *pp) {
+  return !pp->pp_link && pp != page_free_list_top;
+}
+
+//
+// Return a page to the free list.
+// (This function should only be called when pp->pp_ref reaches 0.)
+//
+void
+page_free(struct PageInfo *pp) {
+  // LAB 6: Fill this function in
+  // Hint: You may want to panic if pp->pp_ref is nonzero or
+  // pp->pp_link is not NULL.
+  if (pp->pp_ref != 0 || pp->pp_link != NULL)
+    panic("page_free: Page cannot be freed!\n");
+
+  pp->pp_link    = page_free_list;
+  page_free_list = pp;
+  if (!page_free_list_top) {
+    page_free_list_top = pp;
+  }
+}
+
+//
+// Decrement the reference count on a page,
+// freeing it if there are no more refs.
+//
+void
+page_decref(struct PageInfo *pp) {
+  if (--pp->pp_ref == 0)
+    page_free(pp);
+}
+
+// Given 'pgdir', a pointer to a page directory, pgdir_walk returns
+// a pointer to the page table entry (PTE) for linear address 'va'.
+// This requires walking the two-level page table structure.
+//
+// The relevant page table page might not exist yet.
+// If this is true, and create == false, then pgdir_walk returns NULL.
+// Otherwise, pgdir_walk allocates a new page table page with page_alloc.
+//    - If the allocation fails, pgdir_walk returns NULL.
+//    - Otherwise, the new page's reference count is incremented,
+//	the page is cleared,
+//	and pgdir_walk returns a pointer into the new page table page.
+//
+// Hint 1: you can turn a Page * into the physical address of the
+// page it refers to with page2pa() from kern/pmap.h.
+//
+// Hint 2: the x86 MMU checks permission bits in both the page directory
+// and the page table, so it's safe to leave permissions in the page
+// directory more permissive than strictly necessary.
+//
+// Hint 3: look at inc/mmu.h for useful macros that mainipulate page
+// table and page directory entries.
+//
+pte_t *
+pml4e_walk(pml4e_t *pml4e, const void *va, int create) {
+  // LAB 7: Fill this function in
+  return NULL;
+}
+
+pte_t *
+pdpe_walk(pdpe_t *pdpe, const void *va, int create) {
+  // LAB 7: Fill this function in
+  return NULL;
+}
+
+pte_t *
+pgdir_walk(pde_t *pgdir, const void *va, int create) {
+  // LAB 7: Fill this function in
+  return NULL;
+}
+
+//
+// Map [va, va+size) of virtual address space to physical [pa, pa+size)
+// in the page table rooted at pgdir.  Size is a multiple of PGSIZE, and
+// va and pa are both page-aligned.
+// Use permission bits perm|PTE_P for the entries.
+//
+// This function is only intended to set up the ``static'' mappings
+// above UTOP. As such, it should *not* change the pp_ref field on the
+// mapped pages.
+//
+// Hint: the TA solution uses pgdir_walk
+static void
+boot_map_region(pml4e_t *pml4e, uintptr_t va, size_t size, physaddr_t pa, int perm) {
+  // LAB 7: Fill this function in
+}
+
+//
+// Map the physical page 'pp' at virtual address 'va'.
+// The permissions (the low 12 bits) of the page table entry
+// should be set to 'perm|PTE_P'.
+//
+// Requirements
+//   - If there is already a page mapped at 'va', it should be page_remove()d.
+//   - If necessary, on demand, a page table should be allocated and inserted
+//     into 'pgdir'.
+//   - pp->pp_ref should be incremented if the insertion succeeds.
+//   - The TLB must be invalidated if a page was formerly present at 'va'.
+//
+// Corner-case hint: Make sure to consider what happens when the same
+// pp is re-inserted at the same virtual address in the same pgdir.
+// However, try not to distinguish this case in your code, as this
+// frequently leads to subtle bugs; there's an elegant way to handle
+// everything in one code path.
+//
+// RETURNS:
+//   0 on success
+//   -E_NO_MEM, if page table couldn't be allocated
+//
+// Hint: The TA solution is implemented using pgdir_walk, page_remove,
+// and page2pa.
+//
+int
+page_insert(pml4e_t *pml4e, struct PageInfo *pp, void *va, int perm) {
+  // LAB 7: Fill this function in
+  return 0;
+}
+
+//
+// Return the page mapped at virtual address 'va'.
+// If pte_store is not zero, then we store in it the address
+// of the pte for this page.  This is used by page_remove and
+// can be used to verify page permissions for syscall arguments,
+// but should not be used by most callers.
+//
+// Return NULL if there is no page mapped at va.
+//
+// Hint: the TA solution uses pgdir_walk and pa2page.
+//
+struct PageInfo *
+page_lookup(pml4e_t *pml4e, void *va, pte_t **pte_store) {
+  // LAB 7: Fill this function in
+  return NULL;
+}
+
+//
+// Unmaps the physical page at virtual address 'va'.
+// If there is no physical page at that address, silently does nothing.
+//
+// Details:
+//   - The ref count on the physical page should decrement.
+//   - The physical page should be freed if the refcount reaches 0.
+//   - The pg table entry corresponding to 'va' should be set to 0.
+//     (if such a PTE exists)
+//   - The TLB must be invalidated if you remove an entry from
+//     the page table.
+//
+// Hint: The TA solution is implemented using page_lookup,
+// 	tlb_invalidate, and page_decref.
+//
+void
+page_remove(pml4e_t *pml4e, void *va) {
+  // LAB 7: Fill this function in
+}
+
+//
+// Invalidate a TLB entry, but only if the page tables being
+// edited are the ones currently in use by the processor.
+//
+void
+tlb_invalidate(pml4e_t *pml4e, void *va) {
+  // Flush the entry only if we're modifying the current address space.
+  // For now, there is only one address space, so always invalidate.
+  invlpg(va);
+}
+
+//
+// Reserve size bytes in the MMIO region and map [pa,pa+size) at this
+// location.  Return the base of the reserved region.  size does *not*
+// have to be multiple of PGSIZE.
+//
+void *
+mmio_map_region(physaddr_t pa, size_t size) {
+  // Where to start the next region.  Initially, this is the
+  // beginning of the MMIO region.  Because this is static, its
+  // value will be preserved between calls to mmio_map_region
+  // (just like nextfree in boot_alloc).
+  static uintptr_t base = MMIOBASE;
+
+  // Reserve size bytes of virtual memory starting at base and
+  // map physical pages [pa,pa+size) to virtual addresses
+  // [base,base+size).  Since this is device memory and not
+  // regular DRAM, you'll have to tell the CPU that it isn't
+  // safe to cache access to this memory.  Luckily, the page
+  // tables provide bits for this purpose; simply create the
+  // mapping with PTE_PCD|PTE_PWT (cache-disable and
+  // write-through) in addition to PTE_W.  (If you're interested
+  // in more details on this, see section 10.5 of IA32 volume
+  // 3A.)
+  //
+  // Be sure to round size up to a multiple of PGSIZE and to
+  // handle if this reservation would overflow MMIOLIM (it's
+  // okay to simply panic if this happens).
+  //
+  // Hint: The staff solution uses boot_map_region.
+  //
+  // LAB 6: Your code here:
+
+  (void)base;
+  return NULL;
+}
+
+// --------------------------------------------------------------
+// Checking functions.
+// --------------------------------------------------------------
+
+//
+// Check that the pages on the page_free_list are reasonable.
+//
+static void
+check_page_free_list(bool only_low_memory) {
+  struct PageInfo *pp;
+  unsigned pdx_limit = only_low_memory ? 1 : NPDENTRIES;
+  int nfree_basemem = 0, nfree_extmem = 0;
+  char *first_free_page;
+
+  if (!page_free_list)
+    panic("'page_free_list' is a null pointer!");
+
+  if (only_low_memory) {
+    // Move pages with lower addresses first in the free
+    // list, since entry_pgdir does not map all pages.
+    struct PageInfo *pp1, *pp2;
+    struct PageInfo **tp[2] = {&pp1, &pp2};
+    for (pp = page_free_list; pp; pp = pp->pp_link) {
+      int pagetype  = VPN(page2pa(pp)) >= pdx_limit;
+      *tp[pagetype] = pp;
+      tp[pagetype]  = &pp->pp_link;
+    }
+    *tp[1]         = 0;
+    *tp[0]         = pp2;
+    page_free_list = pp1;
+  }
+
+  // if there's a page that shouldn't be on the free list,
+  // try to make sure it eventually causes trouble.
+  /*for (pp = page_free_list; pp; pp = pp->pp_link) {
+		if (VPN(page2pa(pp)) < pdx_limit) {
+#ifdef SANITIZE_SHADOW_BASE
+			// This is technically invalid memory, access it via unsanitized routine.
+			__nosan_memset(page2kva(pp), 0x97, 128);
+#else
+			memset(page2kva(pp), 0x97, 128);
+#endif
+		}
+	}*/
+
+  first_free_page = (char *)boot_alloc(0);
+  for (pp = page_free_list; pp; pp = pp->pp_link) {
+    // check that we didn't corrupt the free list itself
+    assert(pp >= pages);
+    assert(pp < pages + npages);
+    assert(((char *)pp - (char *)pages) % sizeof(*pp) == 0);
+
+    // check a few pages that shouldn't be on the free list
+    assert(page2pa(pp) != 0);
+    assert(page2pa(pp) != IOPHYSMEM);
+    assert(page2pa(pp) != EXTPHYSMEM - PGSIZE);
+    assert(page2pa(pp) != EXTPHYSMEM);
+    assert(page2pa(pp) < EXTPHYSMEM || (char *)page2kva(pp) >= first_free_page);
+
+    if (page2pa(pp) < EXTPHYSMEM)
+      ++nfree_basemem;
+    else
+      ++nfree_extmem;
+  }
+
+  //assert(nfree_basemem > 0);
+  assert(nfree_extmem > 0);
+}
+
+//
+// Check the physical page allocator (page_alloc(), page_free(),
+// and page_init()).
+//
+static void
+check_page_alloc(void) {
+  struct PageInfo *pp, *pp0, *pp1, *pp2;
+  int nfree;
+  struct PageInfo *fl;
+  char *c;
+  int i;
+
+  if (!pages)
+    panic("'pages' is a null pointer!");
+
+  // check number of free pages
+  for (pp = page_free_list, nfree = 0; pp; pp = pp->pp_link)
+    ++nfree;
+
+  // should be able to allocate three pages
+  pp0 = pp1 = pp2 = 0;
+  assert((pp0 = page_alloc(0)));
+  assert((pp1 = page_alloc(0)));
+  assert((pp2 = page_alloc(0)));
+
+  assert(pp0);
+  assert(pp1 && pp1 != pp0);
+  assert(pp2 && pp2 != pp1 && pp2 != pp0);
+  assert(page2pa(pp0) < npages * PGSIZE);
+  assert(page2pa(pp1) < npages * PGSIZE);
+  assert(page2pa(pp2) < npages * PGSIZE);
+
+  // temporarily steal the rest of the free pages
+  fl             = page_free_list;
+  page_free_list = 0;
+
+  // should be no free memory
+  assert(!page_alloc(0));
+
+  // free and re-allocate?
+  page_free(pp0);
+  page_free(pp1);
+  page_free(pp2);
+  pp0 = pp1 = pp2 = 0;
+  assert((pp0 = page_alloc(0)));
+  assert((pp1 = page_alloc(0)));
+  assert((pp2 = page_alloc(0)));
+  assert(pp0);
+  assert(pp1 && pp1 != pp0);
+  assert(pp2 && pp2 != pp1 && pp2 != pp0);
+  assert(!page_alloc(0));
+
+  // test flags
+  memset(page2kva(pp0), 1, PGSIZE);
+  page_free(pp0);
+  assert((pp = page_alloc(ALLOC_ZERO)));
+  assert(pp && pp0 == pp);
+  c = page2kva(pp);
+  for (i = 0; i < PGSIZE; i++)
+    assert(c[i] == 0);
+
+  // give free list back
+  page_free_list = fl;
+
+  // free the pages we took
+  page_free(pp0);
+  page_free(pp1);
+  page_free(pp2);
+
+  // number of free pages should be the same
+  for (pp = page_free_list; pp; pp = pp->pp_link)
+    --nfree;
+  assert(nfree == 0);
+
+  cprintf("check_page_alloc() succeeded!\n");
+}
diff --git a/kern/pmap.h b/kern/pmap.h
new file mode 100644
index 0000000..c824be8
--- /dev/null
+++ b/kern/pmap.h
@@ -0,0 +1,101 @@
+/* See COPYRIGHT for copyright information. */
+
+#ifndef JOS_KERN_PMAP_H
+#define JOS_KERN_PMAP_H
+#ifndef JOS_KERNEL
+#error "This is a JOS kernel header; user programs should not #include it"
+#endif
+
+#include <inc/memlayout.h>
+#include <inc/assert.h>
+
+extern char bootstacktop[], bootstack[];
+
+extern struct PageInfo *pages;
+extern size_t npages;
+
+extern pde_t *kern_pml4e;
+
+/* This macro takes a kernel virtual address -- an address that points above
+ * KERNBASE, where the machine's maximum 512MB of physical memory is mapped --
+ * and returns the corresponding physical address.  It panics if you pass it a
+ * non-kernel virtual address.
+ */
+#define PADDR(kva) _paddr(__FILE__, __LINE__, kva)
+
+static inline physaddr_t
+_paddr(const char *file, int line, void *kva) {
+  if ((uint64_t)kva < KERNBASE)
+    _panic(file, line, "PADDR called with invalid kva %p", kva);
+  return (physaddr_t)kva - KERNBASE;
+}
+
+/* This macro takes a physical address and returns the corresponding kernel
+ * virtual address.  It panics if you pass an invalid physical address. */
+#define KADDR(pa) _kaddr(__FILE__, __LINE__, pa)
+//CAUTION: use only before page detection!
+#define _KADDR_NOCHECK(pa) (void *)((physaddr_t)pa + KERNBASE)
+
+static inline void *
+_kaddr(const char *file, int line, physaddr_t pa) {
+  if (PGNUM(pa) >= npages)
+    _panic(file, line, "KADDR called with invalid pa %p", (void *)pa);
+  return (void *)(pa + KERNBASE);
+}
+
+#define X86MASK 0xFFFFFFFF
+/* This macro takes a kernel virtual address and applies 32-bit mask to it.
+ * This is used for mapping required regions in kernel PML table so that
+ * required addresses are accessible in 32-bit uefi. */
+#define X86ADDR(kva) ((kva)&X86MASK)
+
+enum {
+  // For page_alloc, zero the returned physical page.
+  ALLOC_ZERO = 1 << 0,
+};
+
+void mem_init(void);
+
+#ifdef SANITIZE_SHADOW_BASE
+void kasan_mem_init(void);
+#endif
+
+void page_init(void);
+struct PageInfo *page_alloc(int alloc_flags);
+void page_free(struct PageInfo *pp);
+int page_insert(pml4e_t *pml4e, struct PageInfo *pp, void *va, int perm);
+void page_remove(pml4e_t *pml4e, void *va);
+struct PageInfo *page_lookup(pml4e_t *pml4e, void *va, pte_t **pte_store);
+void page_decref(struct PageInfo *pp);
+int page_is_allocated(const struct PageInfo *pp);
+
+void tlb_invalidate(pml4e_t *pml4e, void *va);
+
+void *mmio_map_region(physaddr_t pa, size_t size);
+
+static inline physaddr_t
+page2pa(struct PageInfo *pp) {
+  return (pp - pages) << PGSHIFT;
+}
+
+static inline struct PageInfo *
+pa2page(physaddr_t pa) {
+  if (PPN(pa) >= npages) {
+    cprintf("accessing %lx\n", (unsigned long)pa);
+    panic("pa2page called with invalid pa");
+  }
+  return &pages[PPN(pa)];
+}
+
+static inline void *
+page2kva(struct PageInfo *pp) {
+  return KADDR(page2pa(pp));
+}
+
+pte_t *pgdir_walk(pde_t *pgdir, const void *va, int create);
+
+pte_t *pml4e_walk(pml4e_t *pml4e, const void *va, int create);
+
+pde_t *pdpe_walk(pdpe_t *pdpe, const void *va, int create);
+
+#endif /* !JOS_KERN_PMAP_H */
diff --git a/kern/trapentry.S b/kern/trapentry.S
index 27d24d1..9e2d872 100644
--- a/kern/trapentry.S
+++ b/kern/trapentry.S
@@ -8,9 +8,9 @@
 
 #ifdef CONFIG_KSPACE
 .comm intr_ret_rip, 8
-.comm intr_rbp_reg, 8
 .comm intr_rsp_reg, 8
 .comm intr_cs, 8
+.comm intr_ss, 8
 .comm intr_rflags, 8
 #endif
 
@@ -24,16 +24,15 @@ clock_thdlr:
   popq intr_ret_rip(%rip)
   popq intr_cs(%rip)
   popq intr_rflags(%rip)
-  movq %rbp, intr_rbp_reg(%rip)
-  movq %rsp, intr_rsp_reg(%rip)
-  movq $0x0,%rbp
+  popq intr_rsp_reg(%rip)
+  popq intr_ss(%rip)
   leaq bootstacktop(%rip),%rsp
-  pushq $GD_KD
+  pushq intr_ss(%rip)
   pushq intr_rsp_reg(%rip)
   pushq intr_rflags(%rip)
   pushq intr_cs(%rip)
   pushq intr_ret_rip(%rip)
-  pushq $0
+  pushq $0x0
   pushq $(IRQ_OFFSET + IRQ_CLOCK)
   pushq $0x0 // %ds
   pushq $0x0 // %es
@@ -42,7 +41,7 @@ clock_thdlr:
   pushq %rbx
   pushq %rcx
   pushq %rdx
-  pushq intr_rbp_reg(%rip)
+  pushq %rbp
   pushq %rdi
   pushq %rsi
   pushq %r8
@@ -54,6 +53,7 @@ clock_thdlr:
   pushq %r14
   pushq %r15
 
+  xorl %ebp, %ebp
   movq %rsp,%rdi
   call trap
   jmp .
